âŸ¨ValueâŸ© â† â€¢Import "value.bqn"

#debug â† â€¢BQN âŠ‘ â€¢FLines "debug_weights.txt"

Rand â† {ğ•¤
  (1 -Ëœ 2 Ã— â€¢rand.RangeâŠ¸Ã·) 1 -Ëœ 2 â‹† 32
}

Neuron â† {
  weights â† Valueâˆ˜RandÂ¨ â†•ğ•©
  bias â† Value Rand@

  Call â‡ {
    act â† bias { ğ•¨.Add ğ•© }Â´ weights { ğ•¨.Mul ğ•© }Â¨ ğ•©
    act.Tanh@
  }

  Params â‡ {ğ•¤
    weights âˆ¾ bias
  }
}

Layer â† {
  shapeâ€¿count â† ğ•©
  neurons â† NeuronâŸœshapeÂ¨ â†•count

  Call â‡ {
    (1=â‰ ) â—¶ âŠ¢â€¿âŠ‘ ğ•©âŠ¸{ ğ•©.Call ğ•¨ }Â¨ neurons
  }

  Params â‡ {ğ•¤
    âˆ¾ { ğ•©.Params@ }Â¨ neurons
  }
}

MLP â† {
  layers â† LayerË˜ 2 â†• ğ•©

  Call â‡ {
    ğ•© { ğ•¨.Call ğ•© }Â´ âŒ½ layers
  }

  Params â‡ {ğ•¤
    âˆ¾ { ğ•©.Params@ }Â¨ layers
  }
}

m â† MLP 3â€¿4â€¿4â€¿1
#debug { ğ•©.Da ğ•¨ }Â¨ m.Params@

xs â† Valueâš‡0 âŸ¨2,3,Â¯1âŸ©â€¿âŸ¨3,Â¯1,0.5âŸ©â€¿âŸ¨0.5,1,1âŸ©â€¿âŸ¨1,1,Â¯1âŸ©
ys â† ValueÂ¨ 1â€¿Â¯1â€¿Â¯1â€¿1
yp â† m.CallÂ¨ xs
loss â† { ğ•¨.Add ğ•© }Â´ yp { (ğ•¨.Sub ğ•©).Pow 2 }Â¨ ys

loss.Backward@

{ ğ•©.Da (ğ•©.Data@) + Â¯0.01 Ã— ğ•©.Grad@ }Â¨ m.Params@
â€¢Show loss.Data@

DoThing â† {ğ•¤
  yp â†© m.CallÂ¨ xs
  loss â†© { ğ•¨.Add ğ•© }Â´ yp { (ğ•¨.Sub ğ•©).Pow 2 }Â¨ ys

  { ğ•©.Gr 0 }Â¨ m.Params@
  loss.Backward@

  { ğ•©.Da (ğ•©.Data@) + Â¯0.01 Ã— ğ•©.Grad@ }Â¨ m.Params@
  â€¢Show loss.Data@
}

DoThingâŸ10 @

x1 â† Value 2.0
x1.La "x1"
#x1.Gr Â¯3.0 Ã— 0.5

x2 â† Value 0.0
x2.La "x2"
#x2.Gr 1.0 Ã— 0.5

w1 â† Value Â¯3.0
w1.La "w1"
#w1.Gr 2.0 Ã— 0.5

w2 â† Value 1.0
w2.La "w2"
#w2.Gr 0.0 Ã— 0.5

x1w1 â† x1.Mul w1
#x1w1.Gr 0.5

x2w2 â† x2.Mul w2
#x2w2.Gr 0.5

x1w1x2w2 â† x1w1.Add x2w2
#x1w1x2w2.Gr 0.5

b â† Value 6.8813735870195432
b.La "b"
#b.Gr 0.5

n â† x1w1x2w2.Add b
#n.Gr 1 - 2 â‹†Ëœ (n.Tanh@).Data@

#oo â† n.Tanh@
en â† (n.Mul Value 2).Exp@
oo â† (en.Sub Value 1).Div (en.Add Value 1)

oo.Gr 1
> { ğ•©.Show@ }Â¨ oo.Backward@

